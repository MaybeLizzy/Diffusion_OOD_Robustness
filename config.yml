
batch_size: 16
epochs: 8000
lr: 0.00005
dropout: 0.1

max_steps: 80000
gradient_accumulation_steps: 2
adam_epsilon: !!float 1e-8
max_grad_norm: 1.0
logging_steps: 1000
save_steps: 20000
warmup_steps: 0
warmup_ratio: 0.1