{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Diffusion with PLMs to do OOD Detection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook enables to perform the following steps:\n",
    "\n",
    "- Set parameters\n",
    "\n",
    "- Load and preprocess the data\n",
    "\n",
    "- Calculate the reconstruction loss and maha center of ID train split if the choosing method is 'diffusion+maha'\n",
    "\n",
    "- Calculate the reconstruction loss of ID test split\n",
    "\n",
    "- Calculate the reconstruction loss of OOD test split\n",
    "\n",
    "- Report the AUROC and FAR95 accuracy\n",
    "\n",
    "We report the OOD detection result with IMDB as ID dataset using \"Diffusion\" method in this file, where \"Diffusion\" means we only use the reconstruction loss as the score function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import set_seed\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from data import load\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import torch.nn.functional as F\n",
    "from sklearn.covariance import EmpiricalCovariance\n",
    "import random\n",
    "\n",
    "## Choose an ID dataset, including ['sst2', 'trec', 'imdb', and '20ng']\n",
    "in_domain = 'imdb'\n",
    "\n",
    "## Choose an evaluating method, including ['diffusion', 'diffusion+maha']\n",
    "score = \"diffusion\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "seed = 42\n",
    "lamda = 0.99\n",
    "data_path = './dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed():\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading imdb\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initializing a model (with random weights) from a previously-finetuned roberta model    \n",
    "from transformers.modeling_roberta_diffusion import RobertaForMaskedLM\n",
    "if in_domain == \"trec\":\n",
    "    if score == \"diffusion\":\n",
    "        dev_t = 600\n",
    "        model_name_or_path = \"./model/trec/loss\"\n",
    "    elif score == \"diffusion+maha\":\n",
    "        dev_t = 550\n",
    "        model_name_or_path = \"./model/trec/maha\"\n",
    "    ood_domain = ['rte', 'sst2', '20ng', 'mnli', 'wmt16', 'imdb', 'multi30k']     # out-of-domain dataset  \n",
    "    max_len=None\n",
    "elif in_domain == \"20ng\":\n",
    "    ood_domain = ['rte', 'trec', 'sst2', 'imdb', 'mnli', 'wmt16', 'multi30k']     # out-of-domain dataset\n",
    "    model_name_or_path = \"./model/20ng\"\n",
    "    max_len = None\n",
    "    dev_t = 600\n",
    "elif in_domain==\"sst2\":\n",
    "    ood_domain = ['trec', '20ng', 'rte', 'mnli', 'wmt16', 'multi30k']     # out-of-domain dataset\n",
    "    model_name_or_path = \"./model/sst2\"\n",
    "    max_len = None\n",
    "    dev_t = 600            \n",
    "elif in_domain == \"imdb\":\n",
    "    ood_domain = ['multi30k', 'trec', 'wmt16', 'rte', 'mnli']     # out-of-domain dataset\n",
    "    model_name_or_path = \"./model/imdb\"\n",
    "    max_len = 512   # set max_len = None when OOD dataset is 20NG.\n",
    "    dev_t = 700\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model = RobertaForMaskedLM.from_pretrained(model_name_or_path)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "set_seed()\n",
    "\n",
    "train_dataset, dev_dataset, test_dataset = load(in_domain, tokenizer, max_seq_length=max_len, is_id=True, state=None, ood=True)     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the loss and maha center of ID train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if score != \"diffusion\":\n",
    "    print(\"***** Calculate the loss and maha center of ID train split*****\")\n",
    "    train_loss = 0.0\n",
    "    train_loss_list = []\n",
    "    nb_eval_steps = 0\n",
    "    bank_ = None\n",
    "    label_bank_ = None\n",
    "    for batch in tqdm(train_dataset, desc=\"Evaluating\"):\n",
    "        input_ids = torch.tensor(batch.input_ids).unsqueeze(dim=0).to(device)\n",
    "        labels = torch.tensor([batch.labels]).to(device)\n",
    "        if input_ids.shape[1]>512:\n",
    "            input_ids=input_ids[:,:512]\n",
    "        with torch.no_grad():\n",
    "            nb_eval_steps += 1\n",
    "            outputs, lm_loss = model(input_ids, return_dict=True, state=\"dev\", dev_t=dev_t)\n",
    "            train_loss += outputs.loss.item()                        \n",
    "            train_loss_list.append(float(outputs.loss))\n",
    "            # prepare ood\n",
    "            pooled = outputs.hidden_states                   \n",
    "            pooled=pooled[len(pooled)-2]  \n",
    "            pooled = pooled.mean(1)                   \n",
    "            if bank_ is None:\n",
    "                bank_ = pooled.clone().detach()\n",
    "                label_bank_ = labels.clone().detach()\n",
    "            else:\n",
    "                bank = pooled.clone().detach()\n",
    "                label_bank = labels.clone().detach()\n",
    "                bank_ = torch.cat([bank, bank_], dim=0)\n",
    "                label_bank_ = torch.cat([label_bank, label_bank_], dim=0)            \n",
    "\n",
    "    train_loss = train_loss / nb_eval_steps\n",
    "    train_loss_list.sort(reverse = True)\n",
    "    print(\"train largest, smallest:\",train_loss_list[0], train_loss_list[len(train_loss_list)-1])      \n",
    "    class_mean = bank_.mean(0)         \n",
    "    centered_bank = (bank_ - class_mean).detach().cpu().numpy()         \n",
    "    precision = EmpiricalCovariance().fit(centered_bank).precision_.astype(np.float32)          \n",
    "    class_var = torch.from_numpy(precision).float().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the loss of ID test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Calculate the loss of ID test split *****\n",
      "  Num examples = 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25000/25000 [53:14<00:00,  7.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: {'perplexity': tensor(830611.2500), 'avg loss': 13.629916984605789}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"***** Calculate the loss of ID test split *****\")\n",
    "print(\"  Num examples = {}\".format(len(test_dataset)))\n",
    "eval_loss = 0.0\n",
    "nb_eval_steps = 0\n",
    "loss_list = []          \n",
    "maha_scores = []\n",
    "for batch in tqdm(test_dataset, desc=\"Evaluating\"):\n",
    "    input_ids = torch.tensor(batch.input_ids).unsqueeze(dim=0).to(device)\n",
    "    labels = torch.tensor(batch.labels).to(device)\n",
    "    if input_ids.shape[1]>512:\n",
    "        input_ids=input_ids[:,:512]\n",
    "\n",
    "    with torch.no_grad():            \n",
    "        outputs, lm_loss = model(input_ids, return_dict=True, state=\"dev\", dev_t=dev_t)\n",
    "        loss = float(outputs.loss.item())\n",
    "        eval_loss += loss            \n",
    "        loss_list.append(loss)\n",
    "        nb_eval_steps += 1\n",
    "        if score != \"diffusion\":\n",
    "            # maha score\n",
    "            pooled = outputs.hidden_states\n",
    "            pooled=pooled[len(pooled)-2]\n",
    "            pooled = pooled.mean(1) \n",
    "            maha_score = []\n",
    "            centered_pooled = pooled - class_mean.unsqueeze(0)\n",
    "            ms = torch.diag(centered_pooled @ class_var @ centered_pooled.t())\n",
    "            maha_score.append(ms)\n",
    "            maha_score = torch.stack(maha_score, dim=-1)\n",
    "            maha_score = maha_score.min(-1)[0]\n",
    "            maha_scores.append(float(maha_score))          \n",
    "\n",
    "eval_loss = eval_loss / nb_eval_steps\n",
    "perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "result = {\"perplexity\": perplexity, \"avg loss\": eval_loss}\n",
    "print(\"result:\",result)\n",
    "if len(maha_scores)!=0:\n",
    "    print(sum(maha_scores)/len(maha_scores)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the loss of OOD test split and report the AUROC and FAR95 accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_auroc(key, prediction):\n",
    "    new_key = np.copy(key)\n",
    "    new_key[key == 0] = 0\n",
    "    new_key[key > 0] = 1\n",
    "    return roc_auc_score(new_key, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stable_cumsum(arr, rtol=1e-05, atol=1e-08):\n",
    "    out = np.cumsum(arr, dtype=np.float64)\n",
    "    expected = np.sum(arr, dtype=np.float64)\n",
    "    if not np.allclose(out[-1], expected, rtol=rtol, atol=atol):\n",
    "        raise RuntimeError('cumsum was found to be unstable: '\n",
    "                           'its last element does not correspond to sum')\n",
    "    return out\n",
    "\n",
    "def fpr_and_fdr_at_recall(y_true, y_score, recall_level=0.95, pos_label=1.):\n",
    "    y_true = (y_true == pos_label)\n",
    "\n",
    "    desc_score_indices = np.argsort(y_score, kind=\"mergesort\")[::-1]\n",
    "    y_score = y_score[desc_score_indices]\n",
    "    y_true = y_true[desc_score_indices]\n",
    "\n",
    "    distinct_value_indices = np.where(np.diff(y_score))[0]\n",
    "    threshold_idxs = np.r_[distinct_value_indices, y_true.size - 1]\n",
    "\n",
    "    tps = stable_cumsum(y_true)[threshold_idxs]\n",
    "    fps = 1 + threshold_idxs - tps\n",
    "\n",
    "    thresholds = y_score[threshold_idxs]\n",
    "\n",
    "    recall = tps / tps[-1]\n",
    "\n",
    "    last_ind = tps.searchsorted(tps[-1])\n",
    "    sl = slice(last_ind, None, -1)\n",
    "    recall, fps, tps, thresholds = np.r_[recall[sl], 1], np.r_[fps[sl], 0], np.r_[tps[sl], 0], thresholds[sl]\n",
    "\n",
    "    cutoff = np.argmin(np.abs(recall - recall_level))\n",
    "\n",
    "    return fps[cutoff] / (np.sum(np.logical_not(y_true)))\n",
    "\n",
    "def get_fpr_95(key, prediction):\n",
    "    new_key = np.copy(key)\n",
    "    new_key[key == 0] = 0\n",
    "    new_key[key > 0] = 1\n",
    "    score = fpr_and_fdr_at_recall(new_key, prediction)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(lamda,loss_list,ood_loss_list,maha_scores,ood_maha_scores):\n",
    "    print(\"***** Report the AUROC and FAR95 accuracy *****\")\n",
    "    if score == \"diffusion\":\n",
    "        pred_task = np.array(loss_list)\n",
    "        pred_ood = np.array(ood_loss_list)\n",
    "    elif score == \"diffusion+maha\":            \n",
    "        pred_task = lamda * np.array(loss_list) + (1-lamda) * np.array(maha_scores)\n",
    "        pred_ood = lamda * np.array(ood_loss_list) + (1-lamda) * np.array(ood_maha_scores)\n",
    "\n",
    "    inl = np.zeros_like(pred_task).astype(np.int64)\n",
    "    outl = np.ones_like(pred_ood).astype(np.int64)\n",
    "    scores = np.concatenate([pred_task, pred_ood], axis=0)\n",
    "    labels = np.concatenate([inl, outl], axis=0)\n",
    "    auroc, fpr_95 = get_auroc(labels, scores), get_fpr_95(labels, scores)        \n",
    "\n",
    "    print(\"method:{4}, t={5}, ID:{0}, OOD:{1}, auroc:{2}, fpr_95:{3} \".format(in_domain, ood, auroc, fpr_95, score, dev_t))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi30k\n",
      "Loading multi30k\n",
      "***** Calculate the loss of OOD test split *****\n",
      "  Num examples = 2532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2532/2532 [05:24<00:00,  7.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ood_result: {'ood perplexity': tensor(5.5236e+10), 'avg ood loss': 24.73488240068746}\n",
      "***** Report the AUROC and FAR95 accuracy *****\n",
      "method:diffusion, t=700, ID:imdb, OOD:multi30k, auroc:0.9993499368088468, fpr_95:0.0018 \n",
      "trec\n",
      "Loading trec\n",
      "***** Calculate the loss of OOD test split *****\n",
      "  Num examples = 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [01:03<00:00,  7.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ood_result: {'ood perplexity': tensor(8.1065e+10), 'avg ood loss': 25.118516315460205}\n",
      "***** Report the AUROC and FAR95 accuracy *****\n",
      "method:diffusion, t=700, ID:imdb, OOD:trec, auroc:0.9996883199999999, fpr_95:0.001 \n",
      "wmt16\n",
      "Loading wmt16\n",
      "***** Calculate the loss of OOD test split *****\n",
      "  Num examples = 2999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2999/2999 [06:24<00:00,  7.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ood_result: {'ood perplexity': tensor(3.1967e+10), 'avg ood loss': 24.187963184892197}\n",
      "***** Report the AUROC and FAR95 accuracy *****\n",
      "method:diffusion, t=700, ID:imdb, OOD:wmt16, auroc:0.9971062554184729, fpr_95:0.01272 \n",
      "rte\n",
      "Loading rte\n",
      "***** Calculate the loss of OOD test split *****\n",
      "  Num examples = 3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3000/3000 [06:23<00:00,  7.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ood_result: {'ood perplexity': tensor(4.2058e+09), 'avg ood loss': 22.159727679252626}\n",
      "***** Report the AUROC and FAR95 accuracy *****\n",
      "method:diffusion, t=700, ID:imdb, OOD:rte, auroc:0.9577991666666668, fpr_95:0.18932 \n",
      "mnli\n",
      "Loading mnli\n",
      "***** Calculate the loss of OOD test split *****\n",
      "  Num examples = 19643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 19643/19643 [41:53<00:00,  7.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ood_result: {'ood perplexity': tensor(1.5821e+10), 'avg ood loss': 23.48461963700367}\n",
      "***** Report the AUROC and FAR95 accuracy *****\n",
      "method:diffusion, t=700, ID:imdb, OOD:mnli, auroc:0.9908076301990532, fpr_95:0.04076 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for ood in ood_domain:\n",
    "    print(ood)\n",
    "    if in_domain == \"imdb\":\n",
    "        if ood == \"20ng\":\n",
    "            max_len = None\n",
    "        else:\n",
    "            max_len = 512\n",
    "    ood_train_dataset, ood_dev_dataset, ood_test_dataset = load(ood, tokenizer, max_seq_length=max_len, is_id=True, state=None,ood=False)     \n",
    "\n",
    "    print(\"***** Calculate the loss of OOD test split *****\")\n",
    "    print(\"  Num examples = {}\".format(len(ood_test_dataset)))\n",
    "\n",
    "    ood_eval_loss = 0.0\n",
    "    ood_nb_eval_steps = 0\n",
    "    ood_loss_list = []        \n",
    "    true_ = []\n",
    "    ood_maha_scores = []\n",
    "    for batch in tqdm(ood_test_dataset, desc=\"Evaluating\"):\n",
    "        input_ids = torch.tensor(batch.input_ids).unsqueeze(dim=0).to(device)\n",
    "        labels = torch.tensor(batch.labels).to(device)\n",
    "        true_.append(int(labels))\n",
    "        if input_ids.shape[1]>512:\n",
    "            input_ids=input_ids[:,:512]\n",
    "        with torch.no_grad():                \n",
    "            outputs, lm_loss = model(input_ids, return_dict=True, state=\"dev\", dev_t=dev_t)\n",
    "            loss = float(outputs.loss.item())\n",
    "            ood_eval_loss += loss           \n",
    "            ood_loss_list.append(loss)\n",
    "            ood_nb_eval_steps += 1\n",
    "            if score != \"diffusion\":\n",
    "                # maha score\n",
    "                pooled = outputs.hidden_states\n",
    "                pooled=pooled[len(pooled)-2]\n",
    "                pooled = pooled.mean(1)\n",
    "                maha_score = []\n",
    "                centered_pooled = pooled - class_mean.unsqueeze(0)\n",
    "                ms = torch.diag(centered_pooled @ class_var @ centered_pooled.t())\n",
    "                maha_score.append(ms)\n",
    "                maha_score = torch.stack(maha_score, dim=-1)\n",
    "                maha_score = maha_score.min(-1)[0]\n",
    "                ood_maha_scores.append(float(maha_score))\n",
    "\n",
    "    ood_eval_loss = ood_eval_loss / ood_nb_eval_steps\n",
    "    ood_perplexity = torch.exp(torch.tensor(ood_eval_loss))\n",
    "    ood_result = {\"ood perplexity\": ood_perplexity, \"avg ood loss\": ood_eval_loss}\n",
    "    print(\"ood_result:\",ood_result)\n",
    "\n",
    "    if len(ood_maha_scores)!=0:\n",
    "        print(sum(ood_maha_scores)/len(ood_maha_scores))\n",
    "    \n",
    "    get_accuracy(lamda,loss_list,ood_loss_list,maha_scores,ood_maha_scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
